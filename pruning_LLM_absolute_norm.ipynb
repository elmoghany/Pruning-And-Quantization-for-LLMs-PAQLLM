{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0.dev20241117+cu124\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_name = \"models/llama3-8b/\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#GPU\n",
    "print(torch.__version__)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import textwrap\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.89s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pruned = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"models/original_model.pt\")\n",
    "# print(f'size (GB): {os.path.getsize(\"models/original_model.pt\")/1024e6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Get the number of layers\n",
    "num_layers = len(model.model.layers)\n",
    "print(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters before pruning: 8.03B\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "original_param_count = count_parameters(model)\n",
    "print(f\"Total parameters before pruning: {original_param_count/1e9:0.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning:\n",
      "Layer 0:\n",
      "  q_proj: torch.Size([4096, 4096])\n",
      "  k_proj: torch.Size([1024, 4096])\n",
      "  v_proj: torch.Size([1024, 4096])\n",
      "  o_proj: torch.Size([4096, 4096])\n",
      "##############################################\n",
      "  gate_proj: torch.Size([14336, 4096])\n",
      "    up_proj: torch.Size([14336, 4096])\n",
      "  down_proj: torch.Size([4096, 14336])\n"
     ]
    }
   ],
   "source": [
    "def print_layer_info(model, layer_index):\n",
    "    layer = model.model.layers[layer_index]\n",
    "    print(f\"Layer {layer_index}:\")\n",
    "    print(f\"  q_proj: {layer.self_attn.q_proj.weight.shape}\")\n",
    "    print(f\"  k_proj: {layer.self_attn.k_proj.weight.shape}\")\n",
    "    print(f\"  v_proj: {layer.self_attn.v_proj.weight.shape}\")\n",
    "    print(f\"  o_proj: {layer.self_attn.o_proj.weight.shape}\")\n",
    "    print(\"##############################################\")\n",
    "    print(f\"  gate_proj: {layer.mlp.gate_proj.weight.shape}\")\n",
    "    print(f\"    up_proj: {layer.mlp.up_proj.weight.shape}\")\n",
    "    print(f\"  down_proj: {layer.mlp.down_proj.weight.shape}\")\n",
    "print(\"Before pruning:\")\n",
    "print_layer_info(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
    "    gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
    "    up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
    "    importance_scores = gate_max_abs + up_max_abs\n",
    "    return importance_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_neuron_pairs(mlp, prune_percent):\n",
    "    # Extract the weights from the MLP layers\n",
    "    #  these weights are used to calculate each neuron's\n",
    "    #  importance score in the next step.\n",
    "    gate_weight = mlp.gate_proj.weight.data.float()\n",
    "    up_weight = mlp.up_proj.weight.data.float()\n",
    "\n",
    "    #Compute importance stores. Neurons with higher importance scores\n",
    "    # are considered more important and less likely to be pruned.\n",
    "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
    "\n",
    "    #Store the original number of neurons in the intermediate layer.\n",
    "    original_intermediate_size = gate_weight.size(0)\n",
    "    #Computes the number of neurons to prune.\n",
    "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
    "    #Calculate the number of neurons to keep. The new intermediate size.\n",
    "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
    "\n",
    "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
    "\n",
    "    #Select the neuros to keep, by obtaining the indices to keep.\n",
    "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
    "    indices_to_keep = indices_to_keep.sort().values\n",
    "\n",
    "    #create the new layers\n",
    "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False)\n",
    "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False)\n",
    "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False)\n",
    "\n",
    "    #copy weights to the new layers.\n",
    "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
    "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
    "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
    "\n",
    "    #return new layers and intermediate size.\n",
    "    return new_gate_proj, new_up_proj, new_down_proj, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, prune_percent):\n",
    "    new_intermediate_size = None\n",
    "\n",
    "    #loop for each model layer.\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
    "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
    "        # by accesing layer.mlp.\n",
    "        mlp = layer.mlp\n",
    "\n",
    "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
    "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
    "\n",
    "        #Replace the Origiginal Layers with Pruned Layers.\n",
    "        mlp.gate_proj = new_gate_proj\n",
    "        mlp.up_proj = new_up_proj\n",
    "        mlp.down_proj = new_down_proj\n",
    "\n",
    "        #new_intermediate_size only needs to be set once\n",
    "        if new_intermediate_size is None:\n",
    "            new_intermediate_size = new_size\n",
    "\n",
    "    #Update the model config file.\n",
    "    model.config.intermediate_size = new_intermediate_size\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_percent = 0.8  # Prune 80% of neurons\n",
    "pruned_model = update_model(model, prune_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model parameters: 3520860160\n",
      "Reduction in parameters: 4509401088\n",
      "Percentage of weight savings: 56.16%\n"
     ]
    }
   ],
   "source": [
    "# Recalculate the number of parameters\n",
    "pruned_param_count = count_parameters(pruned_model)\n",
    "reduction_in_params = original_param_count - pruned_param_count\n",
    "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
    "\n",
    "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
    "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
    "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning:\n",
      "Layer 0:\n",
      "  q_proj: torch.Size([4096, 4096])\n",
      "  k_proj: torch.Size([1024, 4096])\n",
      "  v_proj: torch.Size([1024, 4096])\n",
      "  o_proj: torch.Size([4096, 4096])\n",
      "##############################################\n",
      "  gate_proj: torch.Size([2868, 4096])\n",
      "    up_proj: torch.Size([2868, 4096])\n",
      "  down_proj: torch.Size([4096, 2868])\n"
     ]
    }
   ],
   "source": [
    "print(\"After pruning:\")\n",
    "print_layer_info(pruned_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after pruning: 3.52B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters after pruning: {count_parameters(pruned_model)/1e9:0.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage is 13.12GB\n"
     ]
    }
   ],
   "source": [
    "print(f'memory usage is {pruned_model.get_memory_footprint()/1024/1024/1024:0.2f}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning:\n",
      "Layer 0:\n",
      "  q_proj: torch.Size([4096, 4096])\n",
      "  k_proj: torch.Size([1024, 4096])\n",
      "  v_proj: torch.Size([1024, 4096])\n",
      "  o_proj: torch.Size([4096, 4096])\n",
      "##############################################\n",
      "  gate_proj: torch.Size([2868, 4096])\n",
      "    up_proj: torch.Size([2868, 4096])\n",
      "  down_proj: torch.Size([4096, 2868])\n"
     ]
    }
   ],
   "source": [
    "print(\"After pruning:\")\n",
    "print_layer_info(pruned_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after pruning: 3.52B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters after pruning: {count_parameters(pruned_model)/1e9:0.2f}B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=2868, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=2868, bias=False)\n",
       "          (down_proj): Linear(in_features=2868, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shapes:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  input_ids: torch.Size([1, 6])\n",
      "  attention_mask: torch.Size([1, 6])\n",
      "Output tensor shape: torch.Size([1, 106])\n",
      "time taken is = 1.08 min\n",
      "Generated Output:\n",
      "\n",
      "Output 1:\n",
      "capital of france is. The \\(\\maths)The \\(\\This)The \\(\\This)TheTheTheTheTheTheThe\n",
      "TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTh\n",
      "eTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheT\n",
      "heTheTheTheTheTheTheTheTheTheTheTheTheATheATheATheATheATheA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def output_wrapped(output_tokenizer):\n",
    "    print(\"Generated Output:\\n\")\n",
    "    for i, sentence in enumerate(output_tokenizer, 1):\n",
    "        wrapped_sentence = textwrap.fill(sentence, width=80)\n",
    "        print(f\"Output {i}:\\n{wrapped_sentence}\\n\")\n",
    "        \n",
    "def get_outputs(model, inputs, tokenizer, max_new_tokens=200):\n",
    "    print(\"Input tensor shapes:\")\n",
    "    print(f\"  input_ids: {inputs['input_ids'].shape}\")\n",
    "    print(f\"  attention_mask: {inputs['attention_mask'].shape}\")\n",
    "\n",
    "    # Wrap model.generate to track tensor sizes\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.1,\n",
    "        early_stopping=False,  # Can stop before reaching max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    print(f\"Output tensor shape: {outputs.shape}\")\n",
    "    return outputs\n",
    "\n",
    "input_sentences = tokenizer_pruned(\"Tell a short history of humanity with happy ending.\", return_tensors=\"pt\")\n",
    "input_sentences = tokenizer_pruned(\"capital of france is.\", return_tensors=\"pt\")\n",
    "start_time = time.time()\n",
    "model_4b_outputs_sentence = get_outputs(pruned_model, input_sentences, tokenizer_pruned, max_new_tokens=100)\n",
    "end_time = time.time()\n",
    "output_decoded_tokenizer = tokenizer_pruned.batch_decode(model_4b_outputs_sentence, skip_special_tokens=True)\n",
    "print(f'time taken is = {(end_time-start_time)/60:0.2f} min')\n",
    "output_wrapped(output_decoded_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size (GB): , 13.753475708007812\n"
     ]
    }
   ],
   "source": [
    "# size of model\n",
    "torch.save(pruned_model.state_dict(), \"models/pruned.pt\")\n",
    "print(f'size (GB): , {os.path.getsize(\"models/pruned.pt\")/1024e6}')\n",
    "# # os.remove(\"models/temp/temp_delme.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|            model.embed_tokens.weight            | 525336576  |\n",
      "|      model.layers.0.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.0.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.0.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.0.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.0.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.0.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.0.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.0.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.0.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.1.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.1.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.1.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.1.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.1.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.1.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.1.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.1.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.1.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.2.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.2.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.2.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.2.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.2.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.2.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.2.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.2.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.2.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.3.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.3.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.3.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.3.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.3.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.3.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.3.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.3.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.3.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.4.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.4.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.4.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.4.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.4.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.4.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.4.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.4.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.4.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.5.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.5.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.5.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.5.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.5.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.5.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.5.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.5.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.5.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.6.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.6.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.6.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.6.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.6.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.6.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.6.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.6.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.6.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.7.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.7.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.7.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.7.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.7.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.7.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.7.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.7.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.7.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.8.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.8.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.8.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.8.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.8.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.8.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.8.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.8.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.8.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.9.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.9.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.9.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.9.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.9.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.9.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.9.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.9.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.9.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.10.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.10.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.10.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.10.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.10.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.10.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.10.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.10.input_layernorm.weight     |    4096    |\n",
      "| model.layers.10.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.11.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.11.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.11.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.11.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.11.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.11.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.11.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.11.input_layernorm.weight     |    4096    |\n",
      "| model.layers.11.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.12.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.12.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.12.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.12.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.12.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.12.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.12.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.12.input_layernorm.weight     |    4096    |\n",
      "| model.layers.12.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.13.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.13.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.13.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.13.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.13.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.13.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.13.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.13.input_layernorm.weight     |    4096    |\n",
      "| model.layers.13.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.14.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.14.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.14.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.14.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.14.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.14.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.14.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.14.input_layernorm.weight     |    4096    |\n",
      "| model.layers.14.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.15.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.15.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.15.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.15.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.15.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.15.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.15.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.15.input_layernorm.weight     |    4096    |\n",
      "| model.layers.15.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.16.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.16.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.16.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.16.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.16.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.16.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.16.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.16.input_layernorm.weight     |    4096    |\n",
      "| model.layers.16.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.17.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.17.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.17.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.17.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.17.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.17.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.17.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.17.input_layernorm.weight     |    4096    |\n",
      "| model.layers.17.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.18.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.18.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.18.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.18.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.18.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.18.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.18.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.18.input_layernorm.weight     |    4096    |\n",
      "| model.layers.18.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.19.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.19.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.19.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.19.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.19.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.19.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.19.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.19.input_layernorm.weight     |    4096    |\n",
      "| model.layers.19.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.20.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.20.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.20.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.20.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.20.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.20.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.20.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.20.input_layernorm.weight     |    4096    |\n",
      "| model.layers.20.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.21.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.21.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.21.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.21.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.21.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.21.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.21.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.21.input_layernorm.weight     |    4096    |\n",
      "| model.layers.21.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.22.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.22.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.22.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.22.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.22.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.22.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.22.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.22.input_layernorm.weight     |    4096    |\n",
      "| model.layers.22.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.23.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.23.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.23.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.23.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.23.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.23.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.23.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.23.input_layernorm.weight     |    4096    |\n",
      "| model.layers.23.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.24.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.24.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.24.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.24.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.24.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.24.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.24.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.24.input_layernorm.weight     |    4096    |\n",
      "| model.layers.24.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.25.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.25.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.25.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.25.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.25.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.25.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.25.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.25.input_layernorm.weight     |    4096    |\n",
      "| model.layers.25.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.26.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.26.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.26.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.26.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.26.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.26.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.26.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.26.input_layernorm.weight     |    4096    |\n",
      "| model.layers.26.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.27.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.27.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.27.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.27.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.27.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.27.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.27.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.27.input_layernorm.weight     |    4096    |\n",
      "| model.layers.27.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.28.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.28.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.28.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.28.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.28.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.28.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.28.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.28.input_layernorm.weight     |    4096    |\n",
      "| model.layers.28.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.29.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.29.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.29.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.29.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.29.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.29.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.29.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.29.input_layernorm.weight     |    4096    |\n",
      "| model.layers.29.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.30.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.30.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.30.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.30.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.30.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.30.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.30.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.30.input_layernorm.weight     |    4096    |\n",
      "| model.layers.30.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.31.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.31.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.31.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.31.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.31.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.31.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.31.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.31.input_layernorm.weight     |    4096    |\n",
      "| model.layers.31.post_attention_layernorm.weight |    4096    |\n",
      "|                model.norm.weight                |    4096    |\n",
      "|                  lm_head.weight                 | 525336576  |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 3.5B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3520860160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(model)\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params//1000/1000/1000:0.1f}B\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(pruned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# def prune_with_svd(matrix, prune_ratio):\n",
    "#     \"\"\"\n",
    "#     Prune rows from a weight matrix using low-rank approximation.\n",
    "#     \"\"\"\n",
    "#     weight = matrix.weight.data\n",
    "#     rank = int(weight.size(0) * (1 - prune_ratio))\n",
    "#     U, S, Vt = torch.svd_lowrank(weight, q=rank)\n",
    "#     pruned_weight = (U @ torch.diag(S) @ Vt)\n",
    "#     matrix.weight = nn.Parameter(pruned_weight)\n",
    "#     return matrix\n",
    "\n",
    "# def prune_rows(matrix, num_rows_to_keep):\n",
    "#     \"\"\"\n",
    "#     Prune rows in a weight matrix.\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         pruned_weights = matrix.weight[:num_rows_to_keep].clone()\n",
    "#         matrix.weight = nn.Parameter(pruned_weights)\n",
    "#         if matrix.bias is not None:\n",
    "#             pruned_bias = matrix.bias[:num_rows_to_keep].clone()\n",
    "#             matrix.bias = nn.Parameter(pruned_bias)\n",
    "#     return matrix\n",
    "\n",
    "# def prune_transformer_layer_with_svd(layer, prune_ratio):\n",
    "#     \"\"\"\n",
    "#     Prune a transformer layer using SVD-based low-rank approximation.\n",
    "#     \"\"\"\n",
    "#     # Prune attention projections\n",
    "#     layer.self_attn.q_proj = prune_with_svd(layer.self_attn.q_proj, prune_ratio)\n",
    "#     layer.self_attn.k_proj = prune_with_svd(layer.self_attn.k_proj, prune_ratio)\n",
    "#     layer.self_attn.v_proj = prune_with_svd(layer.self_attn.v_proj, prune_ratio)\n",
    "#     layer.self_attn.o_proj = prune_with_svd(layer.self_attn.o_proj, prune_ratio)\n",
    "\n",
    "#     # Adjust MLP layers\n",
    "#     new_q_proj_dim = layer.self_attn.q_proj.weight.size(0)\n",
    "#     layer.mlp.gate_proj = prune_rows(layer.mlp.gate_proj, new_q_proj_dim)\n",
    "#     layer.mlp.up_proj = prune_rows(layer.mlp.up_proj, new_q_proj_dim)\n",
    "#     layer.mlp.down_proj = prune_rows(layer.mlp.down_proj, new_q_proj_dim)\n",
    "\n",
    "#     return layer\n",
    "\n",
    "# def prune_llama_model(model, prune_ratio):\n",
    "#     \"\"\"\n",
    "#     Apply pruning to all transformer layers in the model.\n",
    "#     \"\"\"\n",
    "#     for i, layer in enumerate(model.model.layers):\n",
    "#         print(f\"Pruning Layer {i}...\")\n",
    "#         model.model.layers[i] = prune_transformer_layer_with_svd(layer, prune_ratio)\n",
    "#     return model\n",
    "\n",
    "# # Apply pruning to the model\n",
    "# prune_ratio = 0.5\n",
    "# model_pruned = prune_llama_model(model_pruned, prune_ratio)\n",
    "\n",
    "# # Print the number of parameters after pruning\n",
    "# print(f\"Number of parameters after pruning: {sum(p.numel() for p in model_pruned.parameters())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
