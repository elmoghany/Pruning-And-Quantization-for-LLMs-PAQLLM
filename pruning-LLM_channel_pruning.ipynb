{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elmog\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0.dev20241117+cu124\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from typing import Union, List\n",
    "import copy\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "model_name = \"models/llama3-8b/\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#GPU\n",
    "print(torch.__version__)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import textwrap\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:02<00:00, 15.70s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pruned = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"models/original_model.pt\")\n",
    "# print(f'size (GB): {os.path.getsize(\"models/original_model.pt\")/1024e6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Get the number of layers\n",
    "num_layers = len(model.model.layers)\n",
    "print(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters before pruning: 8.03B\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "original_param_count = count_parameters(model)\n",
    "print(f\"Total parameters before pruning: {original_param_count/1e9:0.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning:\n",
      "Layer 0:\n",
      "  q_proj: torch.Size([4096, 4096])\n",
      "  k_proj: torch.Size([1024, 4096])\n",
      "  v_proj: torch.Size([1024, 4096])\n",
      "  o_proj: torch.Size([4096, 4096])\n",
      "##############################################\n",
      "  gate_proj: torch.Size([14336, 4096])\n",
      "    up_proj: torch.Size([14336, 4096])\n",
      "  down_proj: torch.Size([4096, 14336])\n"
     ]
    }
   ],
   "source": [
    "def print_layer_info(model, layer_index):\n",
    "    layer = model.model.layers[layer_index]\n",
    "    print(f\"Layer {layer_index}:\")\n",
    "    print(f\"  q_proj: {layer.self_attn.q_proj.weight.shape}\")\n",
    "    print(f\"  k_proj: {layer.self_attn.k_proj.weight.shape}\")\n",
    "    print(f\"  v_proj: {layer.self_attn.v_proj.weight.shape}\")\n",
    "    print(f\"  o_proj: {layer.self_attn.o_proj.weight.shape}\")\n",
    "    print(\"##############################################\")\n",
    "    print(f\"  gate_proj: {layer.mlp.gate_proj.weight.shape}\")\n",
    "    print(f\"    up_proj: {layer.mlp.up_proj.weight.shape}\")\n",
    "    print(f\"  down_proj: {layer.mlp.down_proj.weight.shape}\")\n",
    "print(\"Before pruning:\")\n",
    "print_layer_info(model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model MACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_macs(model, inputs) -> int:\n",
    "    return profile_macs(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of channels to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_channels_to_keep(channels: int, prune_ratio: float) -> int:\n",
    "    \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n",
    "    Note that preserve_rate = 1. - prune_ratio\n",
    "    \"\"\"\n",
    "    new_channels = channels * (1-prune_ratio) \n",
    "    ##################### YOUR CODE STARTS HERE #####################\n",
    "    return int(round(new_channels))\n",
    "    ##################### YOUR CODE ENDS HERE #####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Channels by Importance\n",
    "\n",
    "As you can see, removing the first 30% of channels in all layers leads to significant accuracy reduction. One potential method to remedy the issue is to find the **less important** channel weights to remove. A popular criterion for importance is to use the Frobenius norm of the weights corresponding to each input channel:\n",
    "\n",
    "> $importance_{i} = \\|W_{i}\\|_2, \\;\\; i = 0, 1, 2,\\cdots, \\#\\mathrm{in\\_channels}-1$\n",
    "\n",
    "We can sort the channel weights from more important to less important, and then keep the frst $k$ channels for each layer.\n",
    "### calculate Frobenius norm of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort the channels from important to non-important\n",
    "def get_input_channel_importance(weight):\n",
    "    importance_0 = torch.linalg.norm(weight, dim=0) # for mlp\n",
    "    importance_1 = torch.linalg.norm(weight, dim=1) # for mlp\n",
    "    print('importance dim=0 rows: ', importance_0.shape)\n",
    "    print('importance dim=1 cols: ', importance_1.shape)\n",
    "    return importance_1\n",
    "\n",
    "    # in_channels = weight.shape[1]\n",
    "    # importances = []\n",
    "    # compute the importance for each input channel\n",
    "    # for i_c in range(weight.shape[1]):\n",
    "        # channel_weight = weight.detach()[:, i_c]\n",
    "        ##################### YOUR CODE STARTS HERE #####################\n",
    "        # importance = torch.linalg.norm(channel_weight) # for cnn\n",
    "        ##################### YOUR CODE ENDS HERE #####################\n",
    "        # importances.append(importance.view(1))\n",
    "    # return torch.cat(importance, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def apply_channel_sorting(model):\n",
    "    model = copy.deepcopy(model)  # do not modify the original model\n",
    "    # fetch all the conv and bn layers from the backbone\n",
    "    all_gate    = []\n",
    "    all_up      = []\n",
    "    all_down    = []\n",
    "    all_in_ln   = []\n",
    "    all_post_ln  = []\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        for attr_name, target_list in [\n",
    "            (\"gate_proj\", all_gate),\n",
    "            (\"up_proj\", all_up),\n",
    "            (\"down_proj\", all_down),\n",
    "        ]:\n",
    "            if hasattr(layer.mlp, attr_name):\n",
    "                module = getattr(layer.mlp, attr_name)\n",
    "                if hasattr(module, \"weight\") and isinstance(module.weight, nn.Parameter):\n",
    "                    target_list.append(module)\n",
    "        if hasattr(layer, \"input_layernorm\"):\n",
    "            module = getattr(layer, \"input_layernorm\")\n",
    "            if hasattr(module, \"weight\") and isinstance(module.weight, nn.Parameter):\n",
    "                all_in_ln.append(module)\n",
    "        if hasattr(layer, \"post_attention_layernorm\"):\n",
    "            module = getattr(layer, \"post_attention_layernorm\")\n",
    "            if hasattr(module, \"weight\") and isinstance(module.weight, nn.Parameter):\n",
    "                all_post_ln.append(module)\n",
    "    # iterate through layers\n",
    "    for i_gate in range(len(all_gate) - 1):\n",
    "        # each channel sorting index, we need to apply it to:\n",
    "        # - the output dimension of the previous conv\n",
    "        # - the previous BN layer\n",
    "        # - the input dimension of the next conv (we compute importance here)\n",
    "        prev_gate   = all_gate[i_gate]\n",
    "        prev_up     = all_up[i_gate]\n",
    "        prev_down   = all_down[i_gate]\n",
    "        prev_in_ln  = all_in_ln[i_gate]\n",
    "        prev_post_ln= all_post_ln[i_gate]\n",
    "        next_gate   = all_gate[i_gate + 1]\n",
    "        next_up     = all_up[i_gate + 1]\n",
    "        next_down   = all_down[i_gate + 1]\n",
    "        \n",
    "        print(\"1\")\n",
    "        print(\"-=-=-=-=\")\n",
    "        print(f\"next_gate.weight.shape: {next_gate.weight.shape}\")\n",
    "        print(f\"prev_gate.weight.shape: {prev_gate.weight.shape}\")\n",
    "        print(\"-=-=-=-=\")\n",
    "        print(f\"next_up.weight.shape: {next_up.weight.shape}\")\n",
    "        print(f\"prev_up.weight.shape: {prev_up.weight.shape}\")\n",
    "        print(\"-=-=-=-=\")\n",
    "        print(f\"next_down.weight.shape: {next_down.weight.shape}\")\n",
    "        print(f\"prev_down.weight.shape: {prev_down.weight.shape}\")\n",
    "        print(\"-=-=-=-=\")\n",
    "        \n",
    "        print(\"#######################\")\n",
    "        print(\"2\")\n",
    "        # note that we always compute the importance according to input channels\n",
    "        importance_gate = get_input_channel_importance(next_gate.weight)\n",
    "        print(f'importance_gate.shape {importance_gate.shape}')\n",
    "        print(\"-=-=-=-=\")\n",
    "        importance_up   = get_input_channel_importance(next_up.weight)\n",
    "        print(f'importance_up.shape {importance_up.shape}')\n",
    "        print(\"-=-=-=-=\")\n",
    "        importance_down = get_input_channel_importance(next_down.weight)\n",
    "        print(f'importance_down.shape {importance_down.shape}')\n",
    "        print(\"-=-=-=-=\")\n",
    "        \n",
    "        print(\"#######################\")\n",
    "        print('3')\n",
    "        # sorting from large to small\n",
    "        sort_idx_gate   = torch.argsort(importance_gate, descending=True)\n",
    "        sort_idx_up     = torch.argsort(importance_up, descending=True)\n",
    "        sort_idx_down   = torch.argsort(importance_down, descending=True)\n",
    "        # sort_idx_gate = torch.argsort(importance_gate[:next_gate.weight.size(1)], descending=True)\n",
    "        # sort_idx_up   = torch.argsort(importance_up[:next_up.weight.size(1)], descending=True)\n",
    "        # sort_idx_down = torch.argsort(importance_down[:next_down.weight.size(1)], descending=True)\n",
    "        print(f\"sort_idx_gate.shape: {sort_idx_gate.shape}\")\n",
    "        print(f\"sort_idx_gate.max(): {sort_idx_gate.max()}\")\n",
    "        print(f\"sort_idx_gate.min(): {sort_idx_gate.min()}\")\n",
    "        print(\"-=-=-=-=\")\n",
    "        print(f\"sort_idx_up.shape: {sort_idx_up.shape}\")\n",
    "        print(f\"sort_idx_up.max(): {sort_idx_up.max()}\")\n",
    "        print(f\"sort_idx_up.min(): {sort_idx_up.min()}\")\n",
    "        print(\"-=-=-=-=\")\n",
    "        print(f\"sort_idx_down.shape: {sort_idx_down.shape}\")\n",
    "        print(f\"sort_idx_down.max(): {sort_idx_down.max()}\")\n",
    "        print(f\"sort_idx_down.min(): {sort_idx_down.min()}\")\n",
    "        print(\"-=-=-=-=\")\n",
    "\n",
    "        # apply to previous conv and its following bn\n",
    "        print(\"4\")\n",
    "        prev_down.weight.copy_(torch.index_select(\n",
    "            prev_down.weight.detach(), 0, sort_idx_down))\n",
    "        print(\"5\")\n",
    "        print(\"#####################\")\n",
    "        print(f\"prev_down.weight.shape: {prev_down.weight.shape}\")\n",
    "        print(f\"sort_idx_down.shape: {sort_idx_down.shape}\")\n",
    "        print(f'prev_ln: {prev_in_ln.weight.shape}')\n",
    "        for tensor_name in ['weight']:\n",
    "            tensor_to_apply = getattr(prev_in_ln, tensor_name)\n",
    "            tensor_to_apply.copy_(\n",
    "                torch.index_select(tensor_to_apply.detach(), 0, sort_idx_down)\n",
    "            )\n",
    "            tensor_to_apply = getattr(prev_post_ln, tensor_name)\n",
    "            tensor_to_apply.copy_(\n",
    "                torch.index_select(tensor_to_apply.detach(), 0, sort_idx_down)\n",
    "            )\n",
    "        print(\"6\")\n",
    "\n",
    "        # apply to the next_* input (hint: one line of code)\n",
    "        print(f\"next_gate.weight.shape: {next_gate.weight.shape}\")\n",
    "        print(f\"sort_idx_gate.weight.shape: {sort_idx_gate.shape}\")\n",
    "\n",
    "        next_gate.weight.copy_(torch.index_select(\n",
    "            next_gate.weight.detach(), 0, sort_idx_gate))\n",
    "        next_up.weight.copy_(torch.index_select(\n",
    "            next_up.weight.detach(), 0, sort_idx_up))\n",
    "        next_down.weight.copy_(torch.index_select(\n",
    "            next_down.weight.detach(), 0, sort_idx_down))\n",
    "        print(\"7\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune channels naively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def channel_prune(model: nn.Module,\n",
    "                  prune_ratio: Union[List, float]) -> nn.Module:\n",
    "\n",
    "    counters = {\n",
    "        \"gate_proj\": 0,\n",
    "        \"up_proj\": 0,\n",
    "        \"down_proj\": 0\n",
    "    }\n",
    "\n",
    "    # n_conv = len([m for m in model.layers if isinstance(m, nn.Conv2d)])\n",
    "    for attr_name in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            module = getattr(layer.mlp, attr_name)\n",
    "            if (hasattr(module, \"weight\") \n",
    "                and isinstance(getattr(layer.mlp, attr_name).weight, nn.Parameter)):\n",
    "                counters[attr_name] += 1\n",
    "\n",
    "    n_gate_proj = counters[\"gate_proj\"]\n",
    "    n_up_proj = counters[\"up_proj\"]\n",
    "    n_down_proj = counters[\"down_proj\"]\n",
    "\n",
    "    # note that for the ratios, it affects the previous conv output and next\n",
    "    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...\n",
    "    if isinstance(prune_ratio, list):\n",
    "        assert len(prune_ratio) == n_gate_proj - 1\n",
    "    else:  # convert float to list\n",
    "        prune_ratio = [prune_ratio] * (n_gate_proj - 1)\n",
    "\n",
    "\n",
    "    all_gate    = []\n",
    "    all_up      = []\n",
    "    all_down    = []\n",
    "    all_lns      = []\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        for attr_name, target_list in [\n",
    "            (\"gate_proj\", all_gate),\n",
    "            (\"up_proj\", all_up),\n",
    "            (\"down_proj\", all_down),\n",
    "        ]:\n",
    "            if hasattr(layer.mlp, attr_name):\n",
    "                module = getattr(layer.mlp, attr_name)\n",
    "                if hasattr(module, \"weight\") and isinstance(module.weight, nn.Parameter):\n",
    "                    target_list.append(module)\n",
    "        if hasattr(layer, \"input_layernorm\"):\n",
    "            module = getattr(layer, \"input_layernorm\")\n",
    "            if hasattr(module, \"weight\") and isinstance(module.weight, nn.Parameter):\n",
    "                all_lns.append(module)\n",
    "\n",
    "    # Print the collected modules\n",
    "    print(f\"Gate Proj Modules: {all_gate}\")\n",
    "    print(f\"Up Proj Modules: {all_up}\")\n",
    "    print(f\"Down Proj Modules: {all_down}\")\n",
    "\n",
    "    # apply pruning. we naively keep the first k channels\n",
    "    assert len(all_gate) == len(all_up)\n",
    "    assert len(all_gate) == len(all_down)\n",
    "    \n",
    "    for i_ratio, p_ratio in enumerate(prune_ratio):\n",
    "        prev_gate = all_gate[i_ratio]\n",
    "        prev_up = all_up[i_ratio]\n",
    "        prev_down = all_down[i_ratio]\n",
    "        next_gate = all_gate[i_ratio + 1]\n",
    "        next_up = all_up[i_ratio + 1]\n",
    "        next_down = all_down[i_ratio + 1]\n",
    "        next_down = all_down[i_ratio + 1]\n",
    "        prev_ln = all_lns[i_ratio]\n",
    "        original_channels = prev_gate.out_channels  # same as next_gate.in_channels\n",
    "        print('original_channels: ', original_channels)\n",
    "        n_keep = get_num_channels_to_keep(original_channels, p_ratio)\n",
    "\n",
    "        # prune the output of the previous conv and bn\n",
    "        prev_gate.weight.set_(prev_gate.weight.detach()[:n_keep])\n",
    "        prev_up.weight.set_(prev_up.weight.detach()[:n_keep])\n",
    "        prev_down.weight.set_(prev_down.weight.detach()[:n_keep])\n",
    "        prev_ln.weight.set_(prev_ln.weight.detach()[:n_keep])\n",
    "\n",
    "        # prune the input of the next conv (hint: just one line of code)\n",
    "        ##################### YOUR CODE STARTS HERE #####################\n",
    "        next_gate.weight.set_(next_gate.weight.detach()[:,:n_keep])\n",
    "        next_up.weight.set_(next_up.weight.detach()[:,:n_keep])\n",
    "        next_down.weight.set_(next_down.weight.detach()[:,:n_keep])\n",
    "        ##################### YOUR CODE ENDS HERE #####################\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * With sorting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "1\n",
      "-=-=-=-=\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "prev_gate.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_up.weight.shape: torch.Size([14336, 4096])\n",
      "prev_up.weight.shape: torch.Size([14336, 4096])\n",
      "-=-=-=-=\n",
      "next_down.weight.shape: torch.Size([4096, 14336])\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "2\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_gate.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([4096])\n",
      "importance dim=1 cols:  torch.Size([14336])\n",
      "importance_up.shape torch.Size([14336])\n",
      "-=-=-=-=\n",
      "importance dim=0 rows:  torch.Size([14336])\n",
      "importance dim=1 cols:  torch.Size([4096])\n",
      "importance_down.shape torch.Size([4096])\n",
      "-=-=-=-=\n",
      "#######################\n",
      "3\n",
      "sort_idx_gate.shape: torch.Size([14336])\n",
      "sort_idx_gate.max(): 14335\n",
      "sort_idx_gate.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_up.shape: torch.Size([14336])\n",
      "sort_idx_up.max(): 14335\n",
      "sort_idx_up.min(): 0\n",
      "-=-=-=-=\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "sort_idx_down.max(): 4095\n",
      "sort_idx_down.min(): 0\n",
      "-=-=-=-=\n",
      "4\n",
      "5\n",
      "#####################\n",
      "prev_down.weight.shape: torch.Size([4096, 14336])\n",
      "sort_idx_down.shape: torch.Size([4096])\n",
      "prev_ln: torch.Size([4096])\n",
      "6\n",
      "next_gate.weight.shape: torch.Size([14336, 4096])\n",
      "sort_idx_gate.weight.shape: torch.Size([14336])\n",
      "7\n",
      "Gate Proj Modules: [Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False)]\n",
      "Up Proj Modules: [Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False), Linear(in_features=4096, out_features=14336, bias=False)]\n",
      "Down Proj Modules: [Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False), Linear(in_features=14336, out_features=4096, bias=False)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m * With sorting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m sorted_model \u001b[38;5;241m=\u001b[39m apply_channel_sorting(model)\n\u001b[1;32m----> 5\u001b[0m pruned_model \u001b[38;5;241m=\u001b[39m \u001b[43mchannel_prune\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_pruning_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(\" * Without sorting...\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# pruned_model = channel_prune(model, channel_pruning_ratio)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elmog\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 69\u001b[0m, in \u001b[0;36mchannel_prune\u001b[1;34m(model, prune_ratio)\u001b[0m\n\u001b[0;32m     67\u001b[0m next_down \u001b[38;5;241m=\u001b[39m all_down[i_ratio \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     68\u001b[0m prev_ln \u001b[38;5;241m=\u001b[39m all_lns[i_ratio]\n\u001b[1;32m---> 69\u001b[0m original_channels \u001b[38;5;241m=\u001b[39m \u001b[43mprev_gate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m  \u001b[38;5;66;03m# same as next_gate.in_channels\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_channels: \u001b[39m\u001b[38;5;124m'\u001b[39m, original_channels)\n\u001b[0;32m     71\u001b[0m n_keep \u001b[38;5;241m=\u001b[39m get_num_channels_to_keep(original_channels, p_ratio)\n",
      "File \u001b[1;32mc:\\Users\\elmog\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1935\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1934\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1935\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1937\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'out_channels'"
     ]
    }
   ],
   "source": [
    "channel_pruning_ratio = 0.3  # pruned-out ratio\n",
    "\n",
    "print(\" * With sorting...\")\n",
    "sorted_model = apply_channel_sorting(model)\n",
    "pruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n",
    "\n",
    "# print(\" * Without sorting...\")\n",
    "# pruned_model = channel_prune(model, channel_pruning_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the pruned models' accuracy with and without sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the number of parameters after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model parameters: 3520860160\n",
      "Reduction in parameters: 4509401088\n",
      "Percentage of weight savings: 56.16%\n"
     ]
    }
   ],
   "source": [
    "# Recalculate the number of parameters\n",
    "pruned_param_count = count_parameters(pruned_model)\n",
    "reduction_in_params = original_param_count - pruned_param_count\n",
    "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
    "\n",
    "print(f\"Pruned model parameters: {pruned_param_count/1e9:0.2f}\")\n",
    "print(f\"Reduction in parameters: {reduction_in_params/1e9:0.2f}\")\n",
    "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 32, 32)\n",
    "# pruned_macs = get_model_macs(pruned_model, dummy_input)\n",
    "# print(f'pruned macs {pruned_macs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning:\n",
      "Layer 0:\n",
      "  q_proj: torch.Size([4096, 4096])\n",
      "  k_proj: torch.Size([1024, 4096])\n",
      "  v_proj: torch.Size([1024, 4096])\n",
      "  o_proj: torch.Size([4096, 4096])\n",
      "##############################################\n",
      "  gate_proj: torch.Size([2868, 4096])\n",
      "    up_proj: torch.Size([2868, 4096])\n",
      "  down_proj: torch.Size([4096, 2868])\n"
     ]
    }
   ],
   "source": [
    "print(\"After pruning:\")\n",
    "print_layer_info(pruned_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after pruning: 3.52B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters after pruning: {count_parameters(pruned_model)/1e9:0.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage is 13.12GB\n"
     ]
    }
   ],
   "source": [
    "print(f'memory usage is {pruned_model.get_memory_footprint()/1024/1024/1024:0.2f}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning:\n",
      "Layer 0:\n",
      "  q_proj: torch.Size([4096, 4096])\n",
      "  k_proj: torch.Size([1024, 4096])\n",
      "  v_proj: torch.Size([1024, 4096])\n",
      "  o_proj: torch.Size([4096, 4096])\n",
      "##############################################\n",
      "  gate_proj: torch.Size([2868, 4096])\n",
      "    up_proj: torch.Size([2868, 4096])\n",
      "  down_proj: torch.Size([4096, 2868])\n"
     ]
    }
   ],
   "source": [
    "print(\"After pruning:\")\n",
    "print_layer_info(pruned_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after pruning: 3.52B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total parameters after pruning: {count_parameters(pruned_model)/1e9:0.2f}B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=2868, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=2868, bias=False)\n",
       "          (down_proj): Linear(in_features=2868, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shapes:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  input_ids: torch.Size([1, 6])\n",
      "  attention_mask: torch.Size([1, 6])\n",
      "Output tensor shape: torch.Size([1, 106])\n",
      "time taken is = 1.08 min\n",
      "Generated Output:\n",
      "\n",
      "Output 1:\n",
      "capital of france is. The \\(\\maths)The \\(\\This)The \\(\\This)TheTheTheTheTheTheThe\n",
      "TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTh\n",
      "eTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheT\n",
      "heTheTheTheTheTheTheTheTheTheTheTheTheATheATheATheATheATheA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def output_wrapped(output_tokenizer):\n",
    "    print(\"Generated Output:\\n\")\n",
    "    for i, sentence in enumerate(output_tokenizer, 1):\n",
    "        wrapped_sentence = textwrap.fill(sentence, width=80)\n",
    "        print(f\"Output {i}:\\n{wrapped_sentence}\\n\")\n",
    "        \n",
    "def get_outputs(model, inputs, tokenizer, max_new_tokens=200):\n",
    "    print(\"Input tensor shapes:\")\n",
    "    print(f\"  input_ids: {inputs['input_ids'].shape}\")\n",
    "    print(f\"  attention_mask: {inputs['attention_mask'].shape}\")\n",
    "\n",
    "    # Wrap model.generate to track tensor sizes\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.1,\n",
    "        early_stopping=False,  # Can stop before reaching max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    print(f\"Output tensor shape: {outputs.shape}\")\n",
    "    return outputs\n",
    "\n",
    "input_sentences = tokenizer_pruned(\"Tell a short history of humanity with happy ending.\", return_tensors=\"pt\")\n",
    "input_sentences = tokenizer_pruned(\"capital of france is.\", return_tensors=\"pt\")\n",
    "start_time = time.time()\n",
    "model_4b_outputs_sentence = get_outputs(pruned_model, input_sentences, tokenizer_pruned, max_new_tokens=100)\n",
    "end_time = time.time()\n",
    "output_decoded_tokenizer = tokenizer_pruned.batch_decode(model_4b_outputs_sentence, skip_special_tokens=True)\n",
    "print(f'time taken is = {(end_time-start_time)/60:0.2f} min')\n",
    "output_wrapped(output_decoded_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size (GB): , 13.753475708007812\n"
     ]
    }
   ],
   "source": [
    "# size of model\n",
    "torch.save(pruned_model.state_dict(), \"models/pruned.pt\")\n",
    "print(f'size (GB): , {os.path.getsize(\"models/pruned.pt\")/1024e6}')\n",
    "# # os.remove(\"models/temp/temp_delme.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|            model.embed_tokens.weight            | 525336576  |\n",
      "|      model.layers.0.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.0.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.0.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.0.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.0.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.0.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.0.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.0.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.0.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.1.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.1.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.1.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.1.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.1.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.1.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.1.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.1.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.1.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.2.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.2.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.2.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.2.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.2.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.2.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.2.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.2.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.2.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.3.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.3.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.3.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.3.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.3.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.3.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.3.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.3.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.3.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.4.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.4.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.4.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.4.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.4.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.4.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.4.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.4.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.4.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.5.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.5.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.5.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.5.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.5.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.5.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.5.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.5.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.5.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.6.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.6.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.6.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.6.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.6.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.6.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.6.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.6.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.6.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.7.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.7.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.7.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.7.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.7.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.7.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.7.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.7.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.7.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.8.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.8.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.8.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.8.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.8.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.8.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.8.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.8.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.8.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.9.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.9.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.9.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.9.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.9.mlp.gate_proj.weight       |  11747328  |\n",
      "|        model.layers.9.mlp.up_proj.weight        |  11747328  |\n",
      "|       model.layers.9.mlp.down_proj.weight       |  11747328  |\n",
      "|      model.layers.9.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.9.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.10.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.10.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.10.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.10.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.10.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.10.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.10.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.10.input_layernorm.weight     |    4096    |\n",
      "| model.layers.10.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.11.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.11.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.11.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.11.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.11.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.11.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.11.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.11.input_layernorm.weight     |    4096    |\n",
      "| model.layers.11.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.12.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.12.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.12.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.12.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.12.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.12.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.12.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.12.input_layernorm.weight     |    4096    |\n",
      "| model.layers.12.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.13.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.13.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.13.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.13.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.13.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.13.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.13.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.13.input_layernorm.weight     |    4096    |\n",
      "| model.layers.13.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.14.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.14.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.14.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.14.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.14.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.14.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.14.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.14.input_layernorm.weight     |    4096    |\n",
      "| model.layers.14.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.15.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.15.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.15.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.15.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.15.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.15.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.15.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.15.input_layernorm.weight     |    4096    |\n",
      "| model.layers.15.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.16.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.16.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.16.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.16.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.16.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.16.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.16.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.16.input_layernorm.weight     |    4096    |\n",
      "| model.layers.16.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.17.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.17.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.17.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.17.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.17.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.17.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.17.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.17.input_layernorm.weight     |    4096    |\n",
      "| model.layers.17.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.18.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.18.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.18.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.18.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.18.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.18.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.18.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.18.input_layernorm.weight     |    4096    |\n",
      "| model.layers.18.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.19.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.19.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.19.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.19.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.19.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.19.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.19.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.19.input_layernorm.weight     |    4096    |\n",
      "| model.layers.19.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.20.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.20.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.20.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.20.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.20.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.20.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.20.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.20.input_layernorm.weight     |    4096    |\n",
      "| model.layers.20.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.21.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.21.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.21.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.21.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.21.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.21.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.21.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.21.input_layernorm.weight     |    4096    |\n",
      "| model.layers.21.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.22.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.22.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.22.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.22.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.22.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.22.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.22.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.22.input_layernorm.weight     |    4096    |\n",
      "| model.layers.22.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.23.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.23.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.23.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.23.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.23.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.23.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.23.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.23.input_layernorm.weight     |    4096    |\n",
      "| model.layers.23.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.24.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.24.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.24.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.24.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.24.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.24.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.24.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.24.input_layernorm.weight     |    4096    |\n",
      "| model.layers.24.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.25.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.25.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.25.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.25.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.25.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.25.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.25.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.25.input_layernorm.weight     |    4096    |\n",
      "| model.layers.25.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.26.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.26.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.26.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.26.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.26.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.26.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.26.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.26.input_layernorm.weight     |    4096    |\n",
      "| model.layers.26.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.27.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.27.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.27.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.27.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.27.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.27.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.27.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.27.input_layernorm.weight     |    4096    |\n",
      "| model.layers.27.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.28.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.28.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.28.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.28.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.28.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.28.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.28.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.28.input_layernorm.weight     |    4096    |\n",
      "| model.layers.28.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.29.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.29.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.29.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.29.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.29.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.29.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.29.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.29.input_layernorm.weight     |    4096    |\n",
      "| model.layers.29.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.30.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.30.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.30.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.30.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.30.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.30.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.30.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.30.input_layernorm.weight     |    4096    |\n",
      "| model.layers.30.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.31.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.31.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.31.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.31.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.31.mlp.gate_proj.weight      |  11747328  |\n",
      "|        model.layers.31.mlp.up_proj.weight       |  11747328  |\n",
      "|       model.layers.31.mlp.down_proj.weight      |  11747328  |\n",
      "|      model.layers.31.input_layernorm.weight     |    4096    |\n",
      "| model.layers.31.post_attention_layernorm.weight |    4096    |\n",
      "|                model.norm.weight                |    4096    |\n",
      "|                  lm_head.weight                 | 525336576  |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 3.5B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3520860160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(model)\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params//1000/1000/1000:0.1f}B\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(pruned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# def prune_with_svd(matrix, prune_ratio):\n",
    "#     \"\"\"\n",
    "#     Prune rows from a weight matrix using low-rank approximation.\n",
    "#     \"\"\"\n",
    "#     weight = matrix.weight.data\n",
    "#     rank = int(weight.size(0) * (1 - prune_ratio))\n",
    "#     U, S, Vt = torch.svd_lowrank(weight, q=rank)\n",
    "#     pruned_weight = (U @ torch.diag(S) @ Vt)\n",
    "#     matrix.weight = nn.Parameter(pruned_weight)\n",
    "#     return matrix\n",
    "\n",
    "# def prune_rows(matrix, num_rows_to_keep):\n",
    "#     \"\"\"\n",
    "#     Prune rows in a weight matrix.\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         pruned_weights = matrix.weight[:num_rows_to_keep].clone()\n",
    "#         matrix.weight = nn.Parameter(pruned_weights)\n",
    "#         if matrix.bias is not None:\n",
    "#             pruned_bias = matrix.bias[:num_rows_to_keep].clone()\n",
    "#             matrix.bias = nn.Parameter(pruned_bias)\n",
    "#     return matrix\n",
    "\n",
    "# def prune_transformer_layer_with_svd(layer, prune_ratio):\n",
    "#     \"\"\"\n",
    "#     Prune a transformer layer using SVD-based low-rank approximation.\n",
    "#     \"\"\"\n",
    "#     # Prune attention projections\n",
    "#     layer.self_attn.q_proj = prune_with_svd(layer.self_attn.q_proj, prune_ratio)\n",
    "#     layer.self_attn.k_proj = prune_with_svd(layer.self_attn.k_proj, prune_ratio)\n",
    "#     layer.self_attn.v_proj = prune_with_svd(layer.self_attn.v_proj, prune_ratio)\n",
    "#     layer.self_attn.o_proj = prune_with_svd(layer.self_attn.o_proj, prune_ratio)\n",
    "\n",
    "#     # Adjust MLP layers\n",
    "#     new_q_proj_dim = layer.self_attn.q_proj.weight.size(0)\n",
    "#     layer.mlp.gate_proj = prune_rows(layer.mlp.gate_proj, new_q_proj_dim)\n",
    "#     layer.mlp.up_proj = prune_rows(layer.mlp.up_proj, new_q_proj_dim)\n",
    "#     layer.mlp.down_proj = prune_rows(layer.mlp.down_proj, new_q_proj_dim)\n",
    "\n",
    "#     return layer\n",
    "\n",
    "# def prune_llama_model(model, prune_ratio):\n",
    "#     \"\"\"\n",
    "#     Apply pruning to all transformer layers in the model.\n",
    "#     \"\"\"\n",
    "#     for i, layer in enumerate(model.model.layers):\n",
    "#         print(f\"Pruning Layer {i}...\")\n",
    "#         model.model.layers[i] = prune_transformer_layer_with_svd(layer, prune_ratio)\n",
    "#     return model\n",
    "\n",
    "# # Apply pruning to the model\n",
    "# prune_ratio = 0.5\n",
    "# model_pruned = prune_llama_model(model_pruned, prune_ratio)\n",
    "\n",
    "# # Print the number of parameters after pruning\n",
    "# print(f\"Number of parameters after pruning: {sum(p.numel() for p in model_pruned.parameters())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
